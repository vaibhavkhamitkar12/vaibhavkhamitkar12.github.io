{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:51:42.552093Z","iopub.execute_input":"2025-02-25T09:51:42.552467Z","iopub.status.idle":"2025-02-25T09:51:42.559260Z","shell.execute_reply.started":"2025-02-25T09:51:42.552435Z","shell.execute_reply":"2025-02-25T09:51:42.558352Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/again1-processed-data/Processed.csv\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AdamW\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport mlflow\nimport mlflow.pytorch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:09:15.091043Z","iopub.execute_input":"2025-02-25T09:09:15.091517Z","iopub.status.idle":"2025-02-25T09:09:22.127413Z","shell.execute_reply.started":"2025-02-25T09:09:15.091462Z","shell.execute_reply":"2025-02-25T09:09:22.126029Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport mlflow\nimport mlflow.sklearn\nimport mlflow.pytorch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sklearn and imblearn imports\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import VotingClassifier\n\n\n\n# NLTK, Gensim and Hugging Face imports for text processing & LDA / BERT\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom gensim import corpora\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n###############################\n# MLflow Setup Functions\n###############################\ndef create_mlflow_directory():\n    \"\"\"Create MLflow directory in current working directory\"\"\"\n    try:\n        current_dir = os.getcwd()\n        mlruns_dir = os.path.join(current_dir, 'mlruns')\n        os.makedirs(mlruns_dir, exist_ok=True)\n        return mlruns_dir\n    except Exception as e:\n        print(f\"Error creating MLflow directory: {str(e)}\")\n        return None\n\ndef setup_mlflow(mlruns_dir, experiment_name):\n    \"\"\"Setup MLflow tracking URI and experiment\"\"\"\n    try:\n        mlflow.set_tracking_uri(f\"file:{mlruns_dir}\")\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n        if experiment is None:\n            mlflow.create_experiment(experiment_name)\n        mlflow.set_experiment(experiment_name)\n        print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n        print(f\"Experiment: {experiment_name}\")\n        return True\n    except Exception as e:\n        print(f\"Error setting up MLflow: {str(e)}\")\n        return False\n\n###############################\n# Data Loading and Preprocessing\n###############################\ndef load_data(file_path):\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Data file not found at: {file_path}\")\n        df = pd.read_csv(file_path)\n        print(\"Dataset loaded. Shape:\", df.shape)\n        # Adjust based on your dataset; drop or rename columns as needed.\n        # For example, if there is an ID column, drop it:\n        if \"ID\" in df.columns:\n            df.drop(columns=[\"ID\"], inplace=True)\n        return df\n    except Exception as e:\n        print(f\"Error loading data: {str(e)}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:09:22.129758Z","iopub.execute_input":"2025-02-25T09:09:22.130587Z","iopub.status.idle":"2025-02-25T09:10:13.386781Z","shell.execute_reply.started":"2025-02-25T09:09:22.130548Z","shell.execute_reply":"2025-02-25T09:10:13.385670Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"###############################\n# Model 1: LDA-based Feature + RF with SMOTE\n###############################\ndef run_model_1(df, target_col):\n    # Preprocess text for LDA\n    stop_words = set(stopwords.words('english'))\n    def preprocess_text(text):\n        tokens = word_tokenize(text.lower())\n        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n        return tokens\n\n    df['tokens'] = df['Cleaned_Feedback'].apply(preprocess_text)\n    dictionary = corpora.Dictionary(df['tokens'])\n    corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n    num_topics = 3\n    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n    def get_dominant_topic(doc_bow):\n        topics = lda_model.get_document_topics(doc_bow)\n        return max(topics, key=lambda x: x[1])[0]\n    df['Feedback_topic'] = [get_dominant_topic(bow) for bow in corpus]\n    print(\"LDA topics assigned. Distribution:\")\n    print(df['Feedback_topic'].value_counts())\n    \n    # Prepare structured data features (exclude text and tokens)\n    exclude_cols = [\"Cleaned_Feedback\", \"tokens\", target_col]\n    feature_cols = [col for col in df.columns if col not in exclude_cols]\n    for col in feature_cols:\n        if df[col].dtype == 'bool':\n            df[col] = df[col].astype(int)\n    X = pd.get_dummies(df[feature_cols], drop_first=True)\n    y = df[target_col]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Apply SMOTE\n    sm = SMOTE(random_state=42)\n    X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n    print(\"After SMOTE, counts:\")\n    print(pd.Series(y_train_res).value_counts())\n    \n    # Train RandomForest model\n    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    with mlflow.start_run(run_name=\"Model_1_RF_LDA_SMOTE\"):\n        mlflow.log_param(\"model\", \"RandomForest with LDA_Features\")\n        rf_clf.fit(X_train_res, y_train_res)\n        y_pred = rf_clf.predict(X_test_scaled)\n        acc = accuracy_score(y_test, y_pred)\n        mlflow.log_metric(\"accuracy\", acc)\n        mlflow.sklearn.log_model(rf_clf, \"rf_model\")\n        report = classification_report(y_test, y_pred)\n        mlflow.log_text(report, \"classification_report.txt\")\n        print(\"Model 1 Accuracy:\", acc)\n        print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:10:13.388255Z","iopub.execute_input":"2025-02-25T09:10:13.389062Z","iopub.status.idle":"2025-02-25T09:10:13.400530Z","shell.execute_reply.started":"2025-02-25T09:10:13.389028Z","shell.execute_reply":"2025-02-25T09:10:13.399357Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"###############################\n# Model 2: bert-base-uncased for Text Embeddings + Structured Data\n###############################\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import resample\n\ndef balance_dataset(df, target):\n    print(\"Before Balancing:\", Counter(df[target]))\n\n    # If target is categorical, apply oversampling\n    if df[target].dtype == 'object':\n        # Convert categorical labels to numerical\n        df[target] = df[target].astype('category').cat.codes\n    \n    # Separate structured features and target\n    structured_cols = df.drop(columns=['Cleaned_Feedback', target]).columns\n    X_structured = df[structured_cols]\n    y = df[target]\n\n    # Apply SMOTE for structured numerical data\n    smote = SMOTE(sampling_strategy='auto', random_state=42)\n    X_resampled, y_resampled = smote.fit_resample(X_structured, y)\n\n    # Convert back to dataframe\n    df_resampled = pd.DataFrame(X_resampled, columns=structured_cols)\n    df_resampled[target] = y_resampled\n\n    # Re-add the text column by randomly duplicating text from existing samples\n    text_upsampled = resample(df['Cleaned_Feedback'], replace=True, n_samples=len(df_resampled), random_state=42)\n    df_resampled['Cleaned_Feedback'] = text_upsampled.reset_index(drop=True)\n\n    print(\"After Balancing:\", Counter(df_resampled[target]))\n    return df_resampled\n\ndef run_model_4(df, target):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    # Balance the dataset before splitting\n    df_balanced = balance_dataset(df, target)\n\n    df_train, df_test = train_test_split(df_balanced, test_size=0.2, random_state=42)\n    train_dataset = CombinedDataset(df_train.reset_index(drop=True), tokenizer, target)\n    test_dataset = CombinedDataset(df_test.reset_index(drop=True), tokenizer, target)\n\n    BATCH_SIZE = 16\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    structured_dim = df_balanced.drop(columns=['Cleaned_Feedback', target]).shape[1]\n    n_classes = df_balanced[target].nunique()\n    \n    model = CombinedModel(structured_dim=structured_dim, n_classes=n_classes)\n    model = model.to(device)\n\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n    epochs = 3\n    total_steps = len(train_loader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            structured = batch[\"structured\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, structured=structured)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            total_loss += loss.item()\n        \n        print(f\"Model 4 - Epoch {epoch+1}/{epochs} Loss: {total_loss/len(train_loader):.4f}\")\n\n    # Evaluation\n    model.eval()\n    preds = []\n    true_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            structured = batch[\"structured\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, structured=structured)\n            _, predicted = torch.max(outputs, dim=1)\n            preds.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n\n    acc = np.mean(np.array(preds) == np.array(true_labels))\n\n    with mlflow.start_run(run_name=\"Model_4_BERT_Structured\"):\n        mlflow.log_param(\"model\", \"Combined BERT + Structured\")\n        mlflow.log_metric(\"accuracy\", acc)\n        mlflow.pytorch.log_model(model, \"combined_model\")\n        report = classification_report(true_labels, preds)\n        mlflow.log_text(report, \"classification_report.txt\")\n        print(\"Model 4 Accuracy:\", acc)\n        print(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:10:13.459845Z","iopub.execute_input":"2025-02-25T09:10:13.460225Z","iopub.status.idle":"2025-02-25T09:10:13.988036Z","shell.execute_reply.started":"2025-02-25T09:10:13.460195Z","shell.execute_reply":"2025-02-25T09:10:13.986967Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"###############################\n# Model 3: Hospital Reviews Model for Text Embeddings + Structured Data\n###############################\n# We'll use brettclaus/Hospital_Reviews to obtain text embeddings\n\nclass CombinedDataset(Dataset):\n    def __init__(self, df, tokenizer, target, max_len=64):\n        # Drop the text column and target from structured features\n        self.structured = df.drop(columns=['Cleaned_Feedback', target]).values.astype(np.float32)\n        self.texts = df['Cleaned_Feedback'].tolist()\n        self.labels = df[target].values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            \"structured\": torch.tensor(self.structured[idx]),\n            \"input_ids\": inputs[\"input_ids\"].flatten(),\n            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(self.labels[idx])\n        }\n\nclass CombinedModel(nn.Module):\n    def __init__(self, structured_dim, n_classes, dropout_prob=0.3):\n        super(CombinedModel, self).__init__()\n        # Use the brettclaus/Hospital_Reviews model instead of bert-base-uncased\n        self.hospital_model = AutoModelForSequenceClassification.from_pretrained(\"brettclaus/Hospital_Reviews\")\n        # For embeddings, we use the underlying model from the loaded model.\n        # Note: The output dimension is typically available from config.hidden_size.\n        self.text_model = self.hospital_model.base_model  # This gives the underlying transformer model\n        text_output_dim = self.hospital_model.config.hidden_size\n        \n        # Structured branch\n        self.fc_structured = nn.Linear(structured_dim, 32)\n        self.dropout = nn.Dropout(dropout_prob)\n        # Final classification head\n        self.fc = nn.Linear(text_output_dim + 32, n_classes)\n        \n    def forward(self, input_ids, attention_mask, structured):\n        # Get text embeddings from the hospital reviews model\n        # For many Hugging Face models, pooler_output is used as the [CLS] token representation.\n        outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.pooler_output\n        \n        structured_out = torch.relu(self.fc_structured(structured))\n        combined = torch.cat((cls_output, structured_out), dim=1)\n        combined = self.dropout(combined)\n        return self.fc(combined)\n\ndef run_model_5(df, target):\n    # Use the tokenizer associated with brettclaus/Hospital_Reviews\n    tokenizer = AutoTokenizer.from_pretrained(\"brettclaus/Hospital_Reviews\")\n    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n    train_dataset = CombinedDataset(df_train.reset_index(drop=True), tokenizer, target)\n    test_dataset = CombinedDataset(df_test.reset_index(drop=True), tokenizer, target)\n    \n    BATCH_SIZE = 16\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    structured_dim = df.drop(columns=['Cleaned_Feedback', target]).shape[1]\n    n_classes = df[target].nunique()\n    \n    model = CombinedModel(structured_dim=structured_dim, n_classes=n_classes)\n    model = model.to(device)\n    \n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n    epochs = 3\n    total_steps = len(train_loader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n    \n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            structured = batch[\"structured\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, structured=structured)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            total_loss += loss.item()\n        print(f\"Model 4 - Epoch {epoch+1}/{epochs} Loss: {total_loss/len(train_loader):.4f}\")\n    \n    # Evaluation\n    model.eval()\n    preds = []\n    true_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            structured = batch[\"structured\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, structured=structured)\n            _, predicted = torch.max(outputs, dim=1)\n            preds.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    acc = np.mean(np.array(preds) == np.array(true_labels))\n    \n    with mlflow.start_run(run_name=\"Model_4_HospitalReviews_Structured\"):\n        mlflow.log_param(\"model\", \"Combined brettclaus/Hospital_Reviews + Structured\")\n        mlflow.log_metric(\"accuracy\", acc)\n        # Log the PyTorch model with MLflow\n        mlflow.pytorch.log_model(model, \"combined_model\")\n        report = classification_report(true_labels, preds)\n        mlflow.log_text(report, \"classification_report.txt\")\n        print(\"Model 4 Accuracy:\", acc)\n        print(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:10:13.989112Z","iopub.execute_input":"2025-02-25T09:10:13.989445Z","iopub.status.idle":"2025-02-25T09:10:14.012596Z","shell.execute_reply.started":"2025-02-25T09:10:13.989412Z","shell.execute_reply":"2025-02-25T09:10:14.011156Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"###############################\n# Main Execution with MLflow Integration\n###############################\ndef main():\n    mlruns_dir = create_mlflow_directory()\n    if not mlruns_dir:\n        return\n    experiment_name = \"Multi_Model_Integration\"\n    if not setup_mlflow(mlruns_dir, experiment_name):\n        return\n    # Adjust the data path as needed\n    data_path = \"/kaggle/input/again1-processed-data/Processed.csv\"\n    df = load_data(data_path)\n    if df is None:\n        return\n    \n    target_col = \"Sentiment\"  # Adjust as needed\n    \n    # Run each model and track with MLflow\n    print(\"\\nRunning Model 1:\")\n    run_model_1(df.copy(), target_col)\n    \n    #print(\"\\nRunning Model 2:\")\n    #run_model_2(df.copy(), target_col)\n    \n    #print(\"\\nRunning Model 3:\")\n    #run_model_3(df.copy(), target_col)\n    \n    print(\"\\nRunning Model 4:\")\n    run_model_4(df.copy(), target_col)\n\n    print(\"\\nRunning Model 5:\")\n    run_model_5(df.copy(), target_col)\n    \n    print(\"\\nAll models executed. To view MLflow UI, run:\")\n    print(f\"mlflow ui --backend-store-uri file:{mlruns_dir}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:10:14.013816Z","iopub.execute_input":"2025-02-25T09:10:14.014139Z","iopub.status.idle":"2025-02-25T09:40:36.750278Z","shell.execute_reply.started":"2025-02-25T09:10:14.014111Z","shell.execute_reply":"2025-02-25T09:40:36.748660Z"}},"outputs":[{"name":"stdout","text":"MLflow Tracking URI: file:/kaggle/working/mlruns\nExperiment: Multi_Model_Integration\nDataset loaded. Shape: (1000, 11)\n\nRunning Model 1:\nLDA topics assigned. Distribution:\nFeedback_topic\n2    426\n0    338\n1    236\nName: count, dtype: int64\nAfter SMOTE, counts:\nSentiment\n1    383\n2    383\n0    383\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/02/25 09:10:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 1 Accuracy: 0.555\n              precision    recall  f1-score   support\n\n           0       0.21      0.15      0.17        41\n           1       0.63      0.63      0.63        63\n           2       0.60      0.68      0.63        96\n\n    accuracy                           0.56       200\n   macro avg       0.48      0.49      0.48       200\nweighted avg       0.53      0.56      0.54       200\n\n\nRunning Model 2:\nModel 2 - SMOTE distribution:\nSentiment\n1    383\n2    383\n0    383\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/02/25 09:10:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 2 Ensemble Accuracy: 1.0\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        41\n           1       1.00      1.00      1.00        63\n           2       1.00      1.00      1.00        96\n\n    accuracy                           1.00       200\n   macro avg       1.00      1.00      1.00       200\nweighted avg       1.00      1.00      1.00       200\n\n\nRunning Model 3:\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/02/25 09:10:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 3 RF Accuracy: 1.0\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        41\n           1       1.00      1.00      1.00        63\n           2       1.00      1.00      1.00        96\n\n    accuracy                           1.00       200\n   macro avg       1.00      1.00      1.00       200\nweighted avg       1.00      1.00      1.00       200\n\n\nRunning Model 4:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3810c92c662540a7b2634a8ee4d34970"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f474d24ab0f14ef582406d8e8b851254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c41617e92d48379b333186183e54e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ae63ae540fd4ca0828f685e29d718a3"}},"metadata":{}},{"name":"stdout","text":"Before Balancing: Counter({2: 479, 1: 318, 0: 203})\nAfter Balancing: Counter({1: 479, 0: 479, 2: 479})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/881 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba9e80f859eb405e9a0f6b9c0eda5fb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0086972eae6841e1aac0fd55cf06b753"}},"metadata":{}},{"name":"stdout","text":"Model 4 - Epoch 1/3 Loss: 5.3917\nModel 4 - Epoch 2/3 Loss: 4.7310\nModel 4 - Epoch 3/3 Loss: 4.7243\n","output_type":"stream"},{"name":"stderr","text":"2025/02/25 09:27:59 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/02/25 09:28:19 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\u001b[31m2025/02/25 09:28:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 4 Accuracy: 0.3229166666666667\n              precision    recall  f1-score   support\n\n           0       0.32      0.35      0.34       109\n           1       0.31      0.54      0.39        92\n           2       0.56      0.06      0.10        87\n\n    accuracy                           0.32       288\n   macro avg       0.40      0.32      0.28       288\nweighted avg       0.39      0.32      0.28       288\n\n\nRunning Model 5:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"943bec3d98024e978d6f7edced49d459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128d9d712bac4053b1bb66c4bd28e840"}},"metadata":{}},{"name":"stdout","text":"Model 4 - Epoch 1/3 Loss: 4.8483\nModel 4 - Epoch 2/3 Loss: 2.7159\nModel 4 - Epoch 3/3 Loss: 2.1016\n","output_type":"stream"},{"name":"stderr","text":"2025/02/25 09:40:17 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/02/25 09:40:36 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\u001b[31m2025/02/25 09:40:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 4 Accuracy: 0.87\n              precision    recall  f1-score   support\n\n           0       0.61      1.00      0.75        40\n           1       1.00      1.00      1.00        75\n           2       1.00      0.69      0.82        85\n\n    accuracy                           0.87       200\n   macro avg       0.87      0.90      0.86       200\nweighted avg       0.92      0.87      0.87       200\n\n\nAll models executed. To view MLflow UI, run:\nmlflow ui --backend-store-uri file:/kaggle/working/mlruns\n","output_type":"stream"}],"execution_count":9}]}