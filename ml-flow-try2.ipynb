{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:17:51.442263Z","iopub.execute_input":"2025-02-26T06:17:51.442695Z","iopub.status.idle":"2025-02-26T06:17:51.858740Z","shell.execute_reply.started":"2025-02-26T06:17:51.442662Z","shell.execute_reply":"2025-02-26T06:17:51.857734Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/again1-processed-data/Processed.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install mlflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:18:49.414490Z","iopub.execute_input":"2025-02-26T06:18:49.414859Z","iopub.status.idle":"2025-02-26T06:19:05.993464Z","shell.execute_reply.started":"2025-02-26T06:18:49.414829Z","shell.execute_reply":"2025-02-26T06:19:05.992298Z"}},"outputs":[{"name":"stdout","text":"Collecting mlflow\n  Downloading mlflow-2.20.2-py3-none-any.whl.metadata (30 kB)\nCollecting mlflow-skinny==2.20.2 (from mlflow)\n  Downloading mlflow_skinny-2.20.2-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\nRequirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.14.1)\nRequirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.5)\nRequirement already satisfied: numpy<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.26.4)\nRequirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.3)\nCollecting pyarrow<19,>=4.0.0 (from mlflow)\n  Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.36)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (3.1.0)\nCollecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.20.2->mlflow)\n  Downloading databricks_sdk-0.44.1-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (8.5.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (1.29.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (1.29.0)\nRequirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (24.2)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (3.20.3)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (2.11.0a2)\nRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.2->mlflow) (4.12.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\nRequirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (1.9.0)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3->mlflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3->mlflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3->mlflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3->mlflow) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3->mlflow) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3->mlflow) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2025.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\nRequirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.2->mlflow) (2.27.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.2->mlflow) (4.0.11)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.2->mlflow) (3.21.0)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.2->mlflow) (1.2.15)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.2->mlflow) (0.50b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.2->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.2->mlflow) (2.29.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.2->mlflow) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.2->mlflow) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.2->mlflow) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3->mlflow) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3->mlflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.2->mlflow) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.2->mlflow) (5.0.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.2->mlflow) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.2->mlflow) (4.9)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.2->mlflow) (0.6.1)\nDownloading mlflow-2.20.2-py3-none-any.whl (28.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mlflow_skinny-2.20.2-py3-none-any.whl (6.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading databricks_sdk-0.44.1-py3-none-any.whl (648 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.7/648.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nInstalling collected packages: pyarrow, gunicorn, graphql-core, graphql-relay, graphene, databricks-sdk, mlflow-skinny, mlflow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed databricks-sdk-0.44.1 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.20.2 mlflow-skinny-2.20.2 pyarrow-18.1.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AdamW\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport mlflow\nimport mlflow.pytorch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:19:17.364068Z","iopub.execute_input":"2025-02-26T06:19:17.364591Z","iopub.status.idle":"2025-02-26T06:19:22.288678Z","shell.execute_reply.started":"2025-02-26T06:19:17.364547Z","shell.execute_reply":"2025-02-26T06:19:22.287618Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, BatchNormalization, SpatialDropout1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:22:43.326900Z","iopub.execute_input":"2025-02-26T06:22:43.327330Z","iopub.status.idle":"2025-02-26T06:22:43.413366Z","shell.execute_reply.started":"2025-02-26T06:22:43.327297Z","shell.execute_reply":"2025-02-26T06:22:43.412437Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport mlflow\nimport mlflow.sklearn\nimport mlflow.pytorch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sklearn and imblearn imports\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import VotingClassifier\n\n\n\n# NLTK, Gensim and Hugging Face imports for text processing & LDA / BERT\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom gensim import corpora\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n###############################\n# MLflow Setup Functions\n###############################\ndef create_mlflow_directory():\n    \"\"\"Create MLflow directory in current working directory\"\"\"\n    try:\n        current_dir = os.getcwd()\n        mlruns_dir = os.path.join(current_dir, 'mlruns')\n        os.makedirs(mlruns_dir, exist_ok=True)\n        return mlruns_dir\n    except Exception as e:\n        print(f\"Error creating MLflow directory: {str(e)}\")\n        return None\n\ndef setup_mlflow(mlruns_dir, experiment_name):\n    \"\"\"Setup MLflow tracking URI and experiment\"\"\"\n    try:\n        mlflow.set_tracking_uri(f\"file:{mlruns_dir}\")\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n        if experiment is None:\n            mlflow.create_experiment(experiment_name)\n        mlflow.set_experiment(experiment_name)\n        print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n        print(f\"Experiment: {experiment_name}\")\n        return True\n    except Exception as e:\n        print(f\"Error setting up MLflow: {str(e)}\")\n        return False\n\n###############################\n# Data Loading and Preprocessing\n###############################\ndef load_data(file_path):\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Data file not found at: {file_path}\")\n        df = pd.read_csv(file_path)\n        print(\"Dataset loaded. Shape:\", df.shape)\n        # Adjust based on your dataset; drop or rename columns as needed.\n        # For example, if there is an ID column, drop it:\n        if \"ID\" in df.columns:\n            df.drop(columns=[\"ID\"], inplace=True)\n        return df\n    except Exception as e:\n        print(f\"Error loading data: {str(e)}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:19:22.399733Z","iopub.execute_input":"2025-02-26T06:19:22.400711Z","iopub.status.idle":"2025-02-26T06:20:13.371868Z","shell.execute_reply.started":"2025-02-26T06:19:22.400659Z","shell.execute_reply":"2025-02-26T06:20:13.370762Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\"\"\"###############################\n# Model 1: RNN model\n###############################\n\ndef run_model_1(df, target_col):\n    \n    # Shuffle dataset to avoid data leakage\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    # Text Preprocessing\n    tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n    tokenizer.fit_on_texts(df['Cleaned_Feedback'].astype(str))\n    sequences = tokenizer.texts_to_sequences(df['Cleaned_Feedback'].astype(str))\n    X_text = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n    \n    # Convert categorical features to numpy arrays\n    X_numeric = df[['Age', 'Wait_Time', 'Doctor_Rating', 'Gender_Male',]].values.astype(np.int32)\n    \n    # Concatenate text and numeric features\n    X = np.hstack((X_text, X_numeric))\n    print(X[0])\n    print(target_col)\n    # Encode sentiment labels\n    y = df[target_col]\n    \n    # Train-Test Split with shuffle\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Check if train-test split has any overlap\n    train_test_overlap = np.intersect1d(X_train, X_test).size\n    print(f\"Number of overlapping samples in train and test sets: {train_test_overlap}\")\n    \n    # Build Simplified RNN Model\n    model = Sequential([\n        Embedding(input_dim=5000, output_dim=64, input_length=X.shape[1]-4),\n        Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))),\n        Dropout(0.3),\n        Bidirectional(LSTM(16, kernel_regularizer=tf.keras.regularizers.l2(0.01))),\n        Dropout(0.3),\n        Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n        Dense(len(set(y)), activation='softmax')  # Multi-class classification\n    ])\n\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    # Train Model with early stopping\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n    \n    # Evaluate Model\n    loss, accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n    \n    \n    y_pred = model.predict(X_test)\n    y_pred_classes = y_pred.argmax(axis=1)\n\n\n    with mlflow.start_run(run_name=\"Model_1_RNN\"):\n        mlflow.log_param(\"model\", \"Combined brettclaus/Hospital_Reviews + Structured\")\n        mlflow.log_metric(\"accuracy\", accuracy)\n        # Log the PyTorch model with MLflow\n        mlflow.pytorch.log_model(model, \"combined_model\")\n        report = classification_report(y_test, y_pred_classes)\n        mlflow.log_text(report, \"classification_report.txt\")\n        print(\"Model 4 Accuracy:\", accuracy)\n        print(report)\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:27:28.414888Z","iopub.execute_input":"2025-02-26T06:27:28.415276Z","iopub.status.idle":"2025-02-26T06:27:28.426868Z","shell.execute_reply.started":"2025-02-26T06:27:28.415250Z","shell.execute_reply":"2025-02-26T06:27:28.425891Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"###############################\n# Model 2: LSTM model\n###############################\ndef run_model_2(df, target_col):\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # Text Preprocessing\n    tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n    tokenizer.fit_on_texts(df['Cleaned_Feedback'].astype(str))\n    sequences = tokenizer.texts_to_sequences(df['Cleaned_Feedback'].astype(str))\n    X_text = pad_sequences(sequences, maxlen=150, padding='post', truncating='post')\n\n    # Convert categorical features to numpy arrays\n    X_numeric = df[['Age', 'Wait_Time', 'Doctor_Rating', 'Gender_Male', 'Visit_Reason_Emergency', 'Visit_Reason_Follow-up','Visit_Reason_New Consultation',\t'Visit_Reason_Routine Visit'\n    ]].values.astype(np.int32)\n    \n    # Concatenate text and numeric features\n    X = np.hstack((X_text, X_numeric))\n    \n    # Encode sentiment labels\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(df['Sentiment'].astype(str))\n    \n    # Train-Test Split with shuffle\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n    # Check if train-test split has any overlap\n    train_test_overlap = np.intersect1d(X_train, X_test).size\n    print(f\"Number of overlapping samples in train and test sets: {train_test_overlap}\")\n    \n    # Build Improved LSTM Model\n    model = Sequential([\n        Embedding(input_dim=10000, output_dim=128, input_length=X.shape[1]-4),\n        SpatialDropout1D(0.3),\n        Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))),\n        BatchNormalization(),\n        Dropout(0.4),\n        LSTM(32, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n        Dropout(0.3),\n        Dense(len(set(y)), activation='softmax')  # Multi-class classification\n    ])\n    \n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Train Model with early stopping\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n    \n    # Evaluate Model\n    loss, accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n    \n    y_pred = model.predict(X_test)\n    y_pred_classes = y_pred.argmax(axis=1)\n   \n    with mlflow.start_run(run_name=\"Model_2_LSTM\"):\n        mlflow.log_param(\"model\", \"Combined brettclaus/Hospital_Reviews + Structured\")\n        mlflow.log_metric(\"accuracy\", accuracy)\n        # Log the PyTorch model with MLflow\n        mlflow.tensorflow.log_model(model, \"combined_model\")\n        # mlflow.pytorch.log_model(model, \"combined_model\")\n        report = classification_report(y_test, y_pred_classes)\n        mlflow.log_text(report, \"classification_report.txt\")\n        print(\"Model 4 Accuracy:\", accuracy)\n        print(report)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:43:02.266920Z","iopub.execute_input":"2025-02-26T06:43:02.267364Z","iopub.status.idle":"2025-02-26T06:43:02.280590Z","shell.execute_reply.started":"2025-02-26T06:43:02.267323Z","shell.execute_reply":"2025-02-26T06:43:02.279144Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"###############################\n# Model 3: LDA-based Feature + RF with SMOTE\n###############################\ndef run_model_3(df, target_col):\n    # Preprocess text for LDA\n    stop_words = set(stopwords.words('english'))\n    def preprocess_text(text):\n        tokens = word_tokenize(text.lower())\n        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n        return tokens\n\n    df['tokens'] = df['Cleaned_Feedback'].apply(preprocess_text)\n    dictionary = corpora.Dictionary(df['tokens'])\n    corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n    num_topics = 3\n    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n    def get_dominant_topic(doc_bow):\n        topics = lda_model.get_document_topics(doc_bow)\n        return max(topics, key=lambda x: x[1])[0]\n    df['Feedback_topic'] = [get_dominant_topic(bow) for bow in corpus]\n    print(\"LDA topics assigned. Distribution:\")\n    print(df['Feedback_topic'].value_counts())\n    \n    # Prepare structured data features (exclude text and tokens)\n    exclude_cols = [\"Cleaned_Feedback\", \"tokens\", target_col]\n    feature_cols = [col for col in df.columns if col not in exclude_cols]\n    for col in feature_cols:\n        if df[col].dtype == 'bool':\n            df[col] = df[col].astype(int)\n    X = pd.get_dummies(df[feature_cols], drop_first=True)\n    y = df[target_col]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Apply SMOTE\n    sm = SMOTE(random_state=42)\n    X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n    print(\"After SMOTE, counts:\")\n    print(pd.Series(y_train_res).value_counts())\n    \n    # Train RandomForest model\n    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    with mlflow.start_run(run_name=\"Model_3_RF_LDA_SMOTE\"):\n        mlflow.log_param(\"model\", \"RandomForest with LDA_Features\")\n        rf_clf.fit(X_train_res, y_train_res)\n        y_pred = rf_clf.predict(X_test_scaled)\n        acc = accuracy_score(y_test, y_pred)\n        mlflow.log_metric(\"accuracy\", acc)\n        mlflow.sklearn.log_model(rf_clf, \"rf_model\")\n        report = classification_report(y_test, y_pred)\n        mlflow.log_text(report, \"classification_report.txt\")\n        print(\"Model 3 Accuracy:\", acc)\n        print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:20:59.885391Z","iopub.execute_input":"2025-02-26T06:20:59.885727Z","iopub.status.idle":"2025-02-26T06:20:59.896777Z","shell.execute_reply.started":"2025-02-26T06:20:59.885703Z","shell.execute_reply":"2025-02-26T06:20:59.895461Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"###############################\n# Model 4: bert-base-uncased for Text Embeddings + Structured Data\n###############################\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import resample\n\ndef balance_dataset(df, target):\n    print(\"Before Balancing:\", Counter(df[target]))\n\n    # If target is categorical, apply oversampling\n    if df[target].dtype == 'object':\n        # Convert categorical labels to numerical\n        df[target] = df[target].astype('category').cat.codes\n    \n    # Separate structured features and target\n    structured_cols = df.drop(columns=['Cleaned_Feedback', target]).columns\n    X_structured = df[structured_cols]\n    y = df[target]\n\n    # Apply SMOTE for structured numerical data\n    smote = SMOTE(sampling_strategy='auto', random_state=42)\n    X_resampled, y_resampled = smote.fit_resample(X_structured, y)\n\n    # Convert back to dataframe\n    df_resampled = pd.DataFrame(X_resampled, columns=structured_cols)\n    df_resampled[target] = y_resampled\n\n    # Re-add the text column by randomly duplicating text from existing samples\n    text_upsampled = resample(df['Cleaned_Feedback'], replace=True, n_samples=len(df_resampled), random_state=42)\n    df_resampled['Cleaned_Feedback'] = text_upsampled.reset_index(drop=True)\n\n    print(\"After Balancing:\", Counter(df_resampled[target]))\n    return df_resampled\n\ndef run_model_4(df, target):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    # Balance the dataset before splitting\n    df_balanced = balance_dataset(df, target)\n\n    df_train, df_test = train_test_split(df_balanced, test_size=0.2, random_state=42)\n    train_dataset = CombinedDataset(df_train.reset_index(drop=True), tokenizer, target)\n    test_dataset = CombinedDataset(df_test.reset_index(drop=True), tokenizer, target)\n\n    BATCH_SIZE = 16\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    structured_dim = df_balanced.drop(columns=['Cleaned_Feedback', target]).shape[1]\n    n_classes = df_balanced[target].nunique()\n    \n    model = CombinedModel(structured_dim=structured_dim, n_classes=n_classes)\n    model = model.to(device)\n\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n    epochs = 3\n    total_steps = len(train_loader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            structured = batch[\"structured\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, structured=structured)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            total_loss += loss.item()\n        \n        print(f\"Model 4 - Epoch {epoch+1}/{epochs} Loss: {total_loss/len(train_loader):.4f}\")\n\n    # Evaluation\n    model.eval()\n    preds = []\n    true_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            structured = batch[\"structured\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, structured=structured)\n            _, predicted = torch.max(outputs, dim=1)\n            preds.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n\n    acc = np.mean(np.array(preds) == np.array(true_labels))\n\n    with mlflow.start_run(run_name=\"Model_4_BERT_Structured\"):\n        mlflow.log_param(\"model\", \"Combined BERT + Structured\")\n        mlflow.log_metric(\"accuracy\", acc)\n        mlflow.pytorch.log_model(model, \"combined_model\")\n        report = classification_report(true_labels, preds)\n        mlflow.log_text(report, \"classification_report.txt\")\n        print(\"Model 4 Accuracy:\", acc)\n        print(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:21:03.071082Z","iopub.execute_input":"2025-02-26T06:21:03.071520Z","iopub.status.idle":"2025-02-26T06:21:03.088649Z","shell.execute_reply.started":"2025-02-26T06:21:03.071490Z","shell.execute_reply":"2025-02-26T06:21:03.087533Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"###############################\n# Model 5: Hospital Reviews Model for Text Embeddings + Structured Data\n###############################\n# We'll use brettclaus/Hospital_Reviews to obtain text embeddings\n\nclass CombinedDataset(Dataset):\n    def __init__(self, df, tokenizer, target, max_len=64):\n        # Drop the text column and target from structured features\n        self.structured = df.drop(columns=['Cleaned_Feedback', target]).values.astype(np.float32)\n        self.texts = df['Cleaned_Feedback'].tolist()\n        self.labels = df[target].values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            \"structured\": torch.tensor(self.structured[idx]),\n            \"input_ids\": inputs[\"input_ids\"].flatten(),\n            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(self.labels[idx])\n        }\n\nclass CombinedModel(nn.Module):\n    def __init__(self, structured_dim, n_classes, dropout_prob=0.3):\n        super(CombinedModel, self).__init__()\n        # Use the brettclaus/Hospital_Reviews model instead of bert-base-uncased\n        self.hospital_model = AutoModelForSequenceClassification.from_pretrained(\"brettclaus/Hospital_Reviews\")\n        # For embeddings, we use the underlying model from the loaded model.\n        # Note: The output dimension is typically available from config.hidden_size.\n        self.text_model = self.hospital_model.base_model  # This gives the underlying transformer model\n        text_output_dim = self.hospital_model.config.hidden_size\n        \n        # Structured branch\n        self.fc_structured = nn.Linear(structured_dim, 32)\n        self.dropout = nn.Dropout(dropout_prob)\n        # Final classification head\n        self.fc = nn.Linear(text_output_dim + 32, n_classes)\n        \n    def forward(self, input_ids, attention_mask, structured):\n        # Get text embeddings from the hospital reviews model\n        # For many Hugging Face models, pooler_output is used as the [CLS] token representation.\n        outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.pooler_output\n        \n        structured_out = torch.relu(self.fc_structured(structured))\n        combined = torch.cat((cls_output, structured_out), dim=1)\n        combined = self.dropout(combined)\n        return self.fc(combined)\n\ndef run_model_5(df, target):\n    # Use the tokenizer associated with brettclaus/Hospital_Reviews\n    tokenizer = AutoTokenizer.from_pretrained(\"brettclaus/Hospital_Reviews\")\n    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n    train_dataset = CombinedDataset(df_train.reset_index(drop=True), tokenizer, target)\n    test_dataset = CombinedDataset(df_test.reset_index(drop=True), tokenizer, target)\n    \n    BATCH_SIZE = 16\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    structured_dim = df.drop(columns=['Cleaned_Feedback', target]).shape[1]\n    n_classes = df[target].nunique()\n    \n    model = CombinedModel(structured_dim=structured_dim, n_classes=n_classes)\n    model = model.to(device)\n    \n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n    epochs = 3\n    total_steps = len(train_loader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n    \n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            structured = batch[\"structured\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, structured=structured)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            total_loss += loss.item()\n        print(f\"Model 4 - Epoch {epoch+1}/{epochs} Loss: {total_loss/len(train_loader):.4f}\")\n    \n    # Evaluation\n    model.eval()\n    preds = []\n    true_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            structured = batch[\"structured\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, structured=structured)\n            _, predicted = torch.max(outputs, dim=1)\n            preds.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    acc = np.mean(np.array(preds) == np.array(true_labels))\n    \n    with mlflow.start_run(run_name=\"Model_5_PretrainedLLM\"):\n        mlflow.log_param(\"model\", \"Combined brettclaus/Hospital_Reviews + Structured\")\n        mlflow.log_metric(\"accuracy\", acc)\n        # Log the PyTorch model with MLflow\n        mlflow.pytorch.log_model(model, \"combined_model\")\n        report = classification_report(true_labels, preds)\n        mlflow.log_text(report, \"classification_report.txt\")\n        print(\"Model 4 Accuracy:\", acc)\n        print(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:21:10.281297Z","iopub.execute_input":"2025-02-26T06:21:10.281669Z","iopub.status.idle":"2025-02-26T06:21:10.301531Z","shell.execute_reply.started":"2025-02-26T06:21:10.281640Z","shell.execute_reply":"2025-02-26T06:21:10.300220Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"###############################\n# Main Execution with MLflow Integration\n###############################\ndef main():\n    mlruns_dir = create_mlflow_directory()\n    if not mlruns_dir:\n        return\n    experiment_name = \"Multi_Model_Integration\"\n    if not setup_mlflow(mlruns_dir, experiment_name):\n        return\n    # Adjust the data path as needed\n    data_path = \"/kaggle/input/again1-processed-data/Processed.csv\"\n    df = load_data(data_path)\n    if df is None:\n        return\n    \n    target_col = \"Sentiment\"  # Adjust as needed\n    \n    # Run each model and track with MLflow\n    #print(\"\\nRunning Model 1:\")\n    #run_model_1(df.copy(), target_col)\n    \n    print(\"\\nRunning Model 2:\")\n    run_model_2(df.copy(), target_col)\n    \n    print(\"\\nRunning Model 3:\")\n    run_model_3(df.copy(), target_col)\n    \n    print(\"\\nRunning Model 4:\")\n    run_model_4(df.copy(), target_col)\n\n    print(\"\\nRunning Model 5:\")\n    run_model_5(df.copy(), target_col)\n    \n    print(\"\\nAll models executed. To view MLflow UI, run:\")\n    print(f\"mlflow ui --backend-store-uri file:{mlruns_dir}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:43:38.465087Z","iopub.execute_input":"2025-02-26T06:43:38.465630Z","iopub.status.idle":"2025-02-26T07:18:32.714505Z","shell.execute_reply.started":"2025-02-26T06:43:38.465588Z","shell.execute_reply":"2025-02-26T07:18:32.712989Z"}},"outputs":[{"name":"stdout","text":"MLflow Tracking URI: file:/kaggle/working/mlruns\nExperiment: Multi_Model_Integration\nDataset loaded. Shape: (1000, 11)\n\nRunning Model 2:\nNumber of overlapping samples in train and test sets: 58\nEpoch 1/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - accuracy: 0.3339 - loss: 6.1391 - val_accuracy: 0.3150 - val_loss: 4.7678\nEpoch 2/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.4171 - loss: 4.7638 - val_accuracy: 0.3150 - val_loss: 3.9135\nEpoch 3/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 180ms/step - accuracy: 0.3545 - loss: 4.0222 - val_accuracy: 0.4550 - val_loss: 3.3805\nEpoch 4/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.3854 - loss: 3.4870 - val_accuracy: 0.4800 - val_loss: 3.0503\nEpoch 5/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.3314 - loss: 3.1914 - val_accuracy: 0.4800 - val_loss: 2.8266\nEpoch 6/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4200 - loss: 2.8862 - val_accuracy: 0.4800 - val_loss: 2.6545\nEpoch 7/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.3806 - loss: 2.7464 - val_accuracy: 0.4800 - val_loss: 2.5165\nEpoch 8/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.4181 - loss: 2.5401 - val_accuracy: 0.4800 - val_loss: 2.3944\nEpoch 9/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.3913 - loss: 2.4702 - val_accuracy: 0.4800 - val_loss: 2.2920\nEpoch 10/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.4428 - loss: 2.3064 - val_accuracy: 0.4800 - val_loss: 2.1941\nEpoch 11/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - accuracy: 0.4158 - loss: 2.1954 - val_accuracy: 0.4800 - val_loss: 2.1125\nEpoch 12/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4316 - loss: 2.1125 - val_accuracy: 0.4800 - val_loss: 2.0316\nEpoch 13/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.4495 - loss: 2.0183 - val_accuracy: 0.4800 - val_loss: 1.9537\nEpoch 14/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.4401 - loss: 1.9422 - val_accuracy: 0.4800 - val_loss: 1.8807\nEpoch 15/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.4465 - loss: 1.8787 - val_accuracy: 0.4800 - val_loss: 1.8203\nEpoch 16/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4690 - loss: 1.8018 - val_accuracy: 0.4800 - val_loss: 1.7684\nEpoch 17/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4557 - loss: 1.7628 - val_accuracy: 0.4800 - val_loss: 1.7090\nEpoch 18/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4274 - loss: 1.7209 - val_accuracy: 0.4800 - val_loss: 1.6544\nEpoch 19/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 183ms/step - accuracy: 0.4793 - loss: 1.6454 - val_accuracy: 0.4800 - val_loss: 1.6033\nEpoch 20/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4804 - loss: 1.5834 - val_accuracy: 0.4800 - val_loss: 1.5566\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4949 - loss: 1.5501\nTest Accuracy: 48.00%\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n","output_type":"stream"},{"name":"stderr","text":"2025/02/26 06:45:08 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n\u001b[31m2025/02/26 06:45:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 4 Accuracy: 0.47999998927116394\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        41\n           1       0.00      0.00      0.00        63\n           2       0.48      1.00      0.65        96\n\n    accuracy                           0.48       200\n   macro avg       0.16      0.33      0.22       200\nweighted avg       0.23      0.48      0.31       200\n\n\nRunning Model 3:\nLDA topics assigned. Distribution:\nFeedback_topic\n2    426\n0    338\n1    236\nName: count, dtype: int64\nAfter SMOTE, counts:\nSentiment\n1    383\n2    383\n0    383\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/02/26 06:45:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 3 Accuracy: 0.555\n              precision    recall  f1-score   support\n\n           0       0.21      0.15      0.17        41\n           1       0.63      0.63      0.63        63\n           2       0.60      0.68      0.63        96\n\n    accuracy                           0.56       200\n   macro avg       0.48      0.49      0.48       200\nweighted avg       0.53      0.56      0.54       200\n\n\nRunning Model 4:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6fc6eb7c68477db1460d95a1652be0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45e79d6d51fa42bcbee0a34eab4e3e5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6da9020cbc420da9a4dddd7eb39efb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b687a58ef604cbbb6695d203264afcd"}},"metadata":{}},{"name":"stdout","text":"Before Balancing: Counter({2: 479, 1: 318, 0: 203})\nAfter Balancing: Counter({1: 479, 0: 479, 2: 479})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/881 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f617dba90e2e434893b153d48cbc8c08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af58bceee13b4ccf88f56fdd70940b32"}},"metadata":{}},{"name":"stdout","text":"Model 4 - Epoch 1/3 Loss: 5.7604\nModel 4 - Epoch 2/3 Loss: 5.4033\nModel 4 - Epoch 3/3 Loss: 4.8467\n","output_type":"stream"},{"name":"stderr","text":"2025/02/26 07:04:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/02/26 07:04:52 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\u001b[31m2025/02/26 07:04:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 4 Accuracy: 0.3854166666666667\n              precision    recall  f1-score   support\n\n           0       0.41      0.66      0.51       109\n           1       0.34      0.41      0.37        92\n           2       1.00      0.01      0.02        87\n\n    accuracy                           0.39       288\n   macro avg       0.58      0.36      0.30       288\nweighted avg       0.57      0.39      0.32       288\n\n\nRunning Model 5:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9cc900d7c6647079384effd0a58f734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4988bed6b71d454bb0dbc77bff8a689a"}},"metadata":{}},{"name":"stdout","text":"Model 4 - Epoch 1/3 Loss: 6.1434\nModel 4 - Epoch 2/3 Loss: 4.5726\nModel 4 - Epoch 3/3 Loss: 3.5741\n","output_type":"stream"},{"name":"stderr","text":"2025/02/26 07:18:13 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/02/26 07:18:31 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n\u001b[31m2025/02/26 07:18:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Model 4 Accuracy: 0.67\n              precision    recall  f1-score   support\n\n           0       1.00      0.53      0.69        40\n           1       1.00      0.37      0.54        75\n           2       0.56      1.00      0.72        85\n\n    accuracy                           0.67       200\n   macro avg       0.85      0.63      0.65       200\nweighted avg       0.81      0.67      0.65       200\n\n\nAll models executed. To view MLflow UI, run:\nmlflow ui --backend-store-uri file:/kaggle/working/mlruns\n","output_type":"stream"}],"execution_count":31}]}